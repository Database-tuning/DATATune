{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TADA - YCSB AA Workload\n",
    "- Data Prepare (metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A_metrics = pd.read_csv(\"/home/external_metrics.csv\")\n",
    "\n",
    "metrics = A_metrics.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Prepare (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "knob_list = glob.glob(\"/home/my_*.cnf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "for xx in range(len(knob_list)):\n",
    "    path = \"/home/my_{}.cnf\".format(xx)\n",
    "    \n",
    "    a_all = pd.read_csv(path, sep=\"=\", names=['Sample', 'value'], header=2)\n",
    "    a_all = a_all.set_index(\"Sample\")\n",
    "    cur_all_df = a_all.T\n",
    "    \n",
    "    if cnt == 0:\n",
    "        A_config = cur_all_df\n",
    "    else :\n",
    "        A_config = pd.concat([A_config, cur_all_df], axis=0)\n",
    "    cnt += 1\n",
    "A_config = A_config.reset_index()\n",
    "A_config = A_config.drop([\"index\"],axis=1)\n",
    "A_config = A_config.drop(A_config.columns[[0,1]], axis=1)\n",
    "\n",
    "\n",
    "A_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all_samples = config + metrics => AutoEncoder's input : Config + Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pd.concat([A_config,metrics], axis=1)\n",
    "\n",
    "all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_columns = [all_samples.columns[0], all_samples.columns[5],all_samples.columns[10],all_samples.columns[13],\n",
    "                    all_samples.columns[17],all_samples.columns[24],all_samples.columns[30],all_samples.columns[31],\n",
    "                    all_samples.columns[32],all_samples.columns[34],all_samples.columns[36],all_samples.columns[37],\n",
    "                    all_samples.columns[58],all_samples.columns[60],all_samples.columns[64],all_samples.columns[68],\n",
    "                    all_samples.columns[72],all_samples.columns[73],all_samples.columns[74],all_samples.columns[75],\n",
    "                    all_samples.columns[77],all_samples.columns[80],all_samples.columns[82],all_samples.columns[83],\n",
    "                    all_samples.columns[90],all_samples.columns[91],all_samples.columns[92],all_samples.columns[93],\n",
    "                    all_samples.columns[118],all_samples.columns[123],all_samples.columns[124],all_samples.columns[125],\n",
    "                    all_samples.columns[126]]\n",
    "\n",
    "all_columns = all_samples.columns\n",
    "continuous_columns = all_columns.drop(discrete_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(discrete_columns)):\n",
    "    a = discrete_columns[i]\n",
    "    all_samples = all_samples.astype({a:'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(continuous_columns)):\n",
    "    a = continuous_columns[i]\n",
    "    all_samples = all_samples.astype({a:'float'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config - metric prediction (with raw data #1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TabNet\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_all = np.array(A_config)\n",
    "Y_all = np.array(metrics)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,Y_all,test_size=0.2, shuffle=True)\n",
    "\n",
    "y_train_tps = y_train[:,0][:, np.newaxis]\n",
    "y_train_latency = y_train[:,1][:, np.newaxis]\n",
    "y_test_tps = y_test[:,0][:, np.newaxis]\n",
    "y_test_latency = y_test[:,1][:, np.newaxis]\n",
    " \n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "Y_scaler_tps  = StandardScaler().fit(y_train_tps)\n",
    "Y_scaler_latecy = StandardScaler().fit(y_train_latency)\n",
    "\n",
    "scaled_X_train = X_scaler.transform(X_train)\n",
    "scaled_X_test = X_scaler.transform(X_test)\n",
    "\n",
    "scaled_y_train_tps = Y_scaler_tps.transform(y_train_tps)\n",
    "scaled_y_train_latecy = Y_scaler_latecy.transform(y_train_latency)\n",
    "\n",
    "scaled_y_test_tps = Y_scaler_tps.transform(y_test_tps)\n",
    "scaled_y_test_latecy = Y_scaler_latecy.transform(y_test_latency)\n",
    "\n",
    "\n",
    "scaled_y_train = np.concatenate([scaled_y_train_tps, scaled_y_train_latecy], 1)\n",
    "scaled_y_test = np.concatenate([scaled_y_test_tps, scaled_y_test_latecy], 1)\n",
    "\n",
    "\n",
    "regressor = TabNetRegressor(verbose = 10,seed = 42,optimizer_fn=torch.optim.AdamW) ### Basic\n",
    "\n",
    "\n",
    "regressor.fit(X_train=scaled_X_train, y_train=scaled_y_train,\n",
    "              eval_set=[(scaled_X_test, scaled_y_test)],\n",
    "              patience=100, \n",
    "              batch_size = 128,\n",
    "              max_epochs=10000,\n",
    "              eval_metric=['mse'])\n",
    "\n",
    "\n",
    "predictions = regressor.predict(scaled_X_test)\n",
    "\n",
    "test_score = mean_squared_error(y_pred = predictions, y_true = scaled_y_test)\n",
    "\n",
    "print('BEST VALID SCORE : ', regressor.best_cost)\n",
    "print('R2 SCORE : ' , r2_score(scaled_y_test, predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#Column 0 :TPS\n",
    "#Column 1 : Latency\n",
    "\n",
    "for i in range(2):  \n",
    "    r2_score_column = r2_score(predictions[:, i], scaled_y_test[:, i])\n",
    "    print(f'Column {i} R2 Score: {r2_score_column}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LHS SAMPLING (make samples #4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "knob_info = pd.read_csv('Knob_Information_MySQL_v5.7.csv')\n",
    "\n",
    "knob_min = knob_info['raw_min']\n",
    "knob_max = knob_info['raw_max']\n",
    "\n",
    "knob_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDOE import *\n",
    "from scipy.stats.distributions import uniform\n",
    "\n",
    "def LH_Sampling(KNOB, KNOB_DETAILS, sample_num):\n",
    "    maxvals = []\n",
    "    minvals = []\n",
    "    types = []\n",
    "    names = []\n",
    "    nfeats = len(KNOB)\n",
    "    \n",
    "    for knob in range (len(KNOB)):\n",
    "        names.append(knob)\n",
    "        knob_info = KNOB_DETAILS\n",
    "        \n",
    "        \n",
    "        if knob_info['type'][knob] == 'boolean':\n",
    "            maxvals.append(int(1))\n",
    "            minvals.append(int(0))\n",
    "        else:\n",
    "            maxvals.append((knob_info['raw_max'][knob]).astype(int))\n",
    "            minvals.append((knob_info['raw_min'][knob]).astype(int))\n",
    "        types.append(knob_info['type'])\n",
    "        \n",
    "    \n",
    "    samples = lhs(nfeats, samples=sample_num, criterion='maximin')\n",
    "    \n",
    "    maxvals = np.array(maxvals)\n",
    "    minvals = np.array(minvals)\n",
    "    scales = maxvals - minvals\n",
    "    \n",
    "    for fidx in range(nfeats):\n",
    "        samples[:, fidx] = uniform(loc=minvals[fidx], scale=scales[fidx]).ppf(samples[:, fidx])\n",
    "        \n",
    "    lhs_samples = []\n",
    "    for sidx in range(sample_num):\n",
    "        lhs_samples.append(dict())\n",
    "        for fidx in range(nfeats):\n",
    "                        lhs_samples[-1][names[fidx]] = int(round(samples[sidx][fidx]))\n",
    "           \n",
    "            \n",
    "    random.shuffle(lhs_samples)\n",
    "\n",
    "    return lhs_samples\n",
    "\n",
    "A_config_columns = A_config.columns.to_list()\n",
    "A_config_columns_stripped = [column.strip() for column in A_config_columns]\n",
    "mm_sample = LH_Sampling(A_config_columns_stripped, knob_info, 4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = np.array(mm_sample)\n",
    "sample_list = []\n",
    "for ll in mm_sample:\n",
    "    val = list(ll.values())\n",
    "    sample_list.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_list\n",
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predict Metrics with New Samples\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "new_X = np.array(samples)\n",
    "Z_scaler = MinMaxScaler().fit(new_X)\n",
    "new_X_ = Z_scaler.transform(new_X)\n",
    "\n",
    "predictions_new = regressor.predict(new_X_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new_df = pd.DataFrame(predictions_new) \n",
    "\n",
    "inverse_new_pred_tps = Y_scaler_tps.inverse_transform(predictions_new[:, 0].reshape(-1, 1))\n",
    "inverse_new_pred_lat = Y_scaler_latecy.inverse_transform(predictions_new[:, 1].reshape(-1, 1))\n",
    "\n",
    "inverse_new_pred_sum = np.concatenate([inverse_new_pred_tps, inverse_new_pred_lat], axis=1)\n",
    "inverse_new_pred_pd = pd.DataFrame(inverse_new_pred_sum)\n",
    "inverse_new_pred_pd.rename(columns={0: \"tps\", 1:\"latency\"}, inplace=True)\n",
    "\n",
    "\n",
    "new_metrics_re = pd.concat([metrics,inverse_new_pred_pd], axis=0)\n",
    "\n",
    "\n",
    "new_metrics_re = new_metrics_re.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics_re = new_metrics_re.drop(['index'], axis=1)\n",
    "new_metrics_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_pd = pd.DataFrame(new_X)\n",
    "\n",
    "for i in range(len(new_X_pd.columns)):\n",
    "    new_X_pd.rename(columns={new_X_pd.columns[i]: A_config.columns[i]}, inplace=True)   \n",
    "    \n",
    "new_Samples = pd.concat([A_config,new_X_pd] , axis=0)\n",
    "new_Samples = new_Samples.reset_index()\n",
    "new_Samples = new_Samples.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### newnewnew == origin data + aug data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newnewwnew = pd.concat([new_Samples, new_metrics_re], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEncoder (raw data + new data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import  TensorDataset, DataLoader\n",
    "\n",
    "scaler_conf = MinMaxScaler()\n",
    "scaler_tps = MinMaxScaler()\n",
    "scaler_lat = MinMaxScaler()\n",
    "\n",
    "scaled_samples = scaler_conf.fit_transform(new_Samples)\n",
    "\n",
    "scaled_new_metrics_re_tps = scaler_tps.fit_transform(new_metrics_re['tps'].values.reshape(-1, 1))\n",
    "scaled_new_metrics_re_lat = scaler_lat.fit_transform(new_metrics_re['latency'].values.reshape(-1, 1))\n",
    "\n",
    "scaled_new_Samples = np.concatenate([scaled_samples,scaled_new_metrics_re_tps,scaled_new_metrics_re_lat], axis = 1)\n",
    "\n",
    "X_train, X_test = train_test_split(scaled_new_Samples, test_size=0.2, shuffle=True)\n",
    "\n",
    "dataset_tr = TensorDataset(torch.tensor(X_train))\n",
    "dataset_te = TensorDataset(torch.tensor(X_test))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset_tr, batch_size=256, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset_te, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(140,128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(128,64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,32),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,32),        \n",
    "        nn.Sigmoid())\n",
    "    \n",
    "    self.decoder = nn.Sequential(\n",
    "  \n",
    "        nn.Linear(32,32),\n",
    "        nn.BatchNorm1d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32,64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 140), \n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import device\n",
    "from torch import optim\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "\n",
    "critertion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "trainloss = []\n",
    "validationloss = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in range(25000):\n",
    "  running_loss = 0\n",
    "  model.train()\n",
    "  \n",
    "  for data in trainloader:\n",
    "    inputs = data[0].float().to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs) \n",
    "    loss = critertion(inputs, outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "  train_loss = running_loss / len(trainloader)\n",
    "  trainloss.append(train_loss)\n",
    "\n",
    "  \n",
    "  if epoch % 300 == 0:\n",
    "    total_val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      running_loss = 0\n",
    "      \n",
    "      for data in testloader:\n",
    "        inputs = data[0].float().to(device)\n",
    "        outputs = model(inputs) \n",
    "        loss = critertion(inputs, outputs)\n",
    "        running_loss += loss.item()\n",
    "        inputs_np = inputs.cpu().detach().numpy()\n",
    "        outputs_np = outputs.cpu().detach().numpy()\n",
    "      total_val_loss = running_loss / len(testloader)\n",
    "      validationloss.append(total_val_loss)\n",
    "        \n",
    "    print('[%d] tr_loss : %.3f | val_loss : %.3f' %(epoch +1, train_loss, total_val_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization in latent space (TabNet)\n",
    "- Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ex_scaled_new_Samples = torch.Tensor(scaled_new_Samples).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_vector_BO = model.encoder(ex_scaled_new_Samples)\n",
    "    print(\"BO'S Encoded Latent Vector:\", encoded_vector_BO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_total = torch.tensor(X_test)\n",
    "recon_total = model(torch.tensor(X_test).to(\"cuda\").float())\n",
    "\n",
    "label_total_np = label_total.detach().cpu().numpy()\n",
    "recon_total_np = recon_total.detach().cpu().numpy()\n",
    "\n",
    "all_concat = np.concatenate([label_total_np,recon_total_np] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Model\n",
    "- input : Latent Space of AE\n",
    "- output : metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TabNet\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_latent = np.array(encoded_vector_BO.cpu().numpy())\n",
    "Y_latent = np.array(new_metrics_re)\n",
    "\n",
    "lt_X_train, lt_X_test, lt_y_train, lt_y_test = train_test_split(X_latent,Y_latent,test_size=0.2, shuffle=True)\n",
    "\n",
    "\n",
    "y_train_tps = lt_y_train[:,0][:, np.newaxis]\n",
    "y_train_latecy = lt_y_train[:,1][:, np.newaxis]\n",
    "y_test_tps = lt_y_test[:,0][:, np.newaxis]\n",
    "y_test_latecy = lt_y_test[:,1][:, np.newaxis]\n",
    "\n",
    "\n",
    "Y_scaler_tps  = MinMaxScaler().fit(y_train_tps)\n",
    "Y_scaler_latecy = MinMaxScaler().fit(y_train_latecy)\n",
    "\n",
    "\n",
    "scaled_lt_y_train_tps = Y_scaler_tps.transform(y_train_tps)\n",
    "scaled_lt_y_train_latency = Y_scaler_latecy.transform(y_train_latecy)\n",
    "\n",
    "\n",
    "scaled_lt_y_test_tps = Y_scaler_tps.transform(y_test_tps)\n",
    "scaled_lt_y_test_latency = Y_scaler_latecy.transform(y_test_latecy)\n",
    "\n",
    "\n",
    "scaled_lt_y_train = np.concatenate([scaled_lt_y_train_tps, scaled_lt_y_train_latency], axis = 1)\n",
    "scaled_lt_y_test = np.concatenate([scaled_lt_y_test_tps, scaled_lt_y_test_latency], axis = 1)\n",
    "\n",
    "lt_regressor = TabNetRegressor(verbose = 10,seed = 42,optimizer_fn=torch.optim.AdamW) \n",
    "    \n",
    "lt_regressor.fit(X_train=lt_X_train, y_train=scaled_lt_y_train,\n",
    "              eval_set=[(lt_X_test, scaled_lt_y_test)],\n",
    "              patience=500, \n",
    "              batch_size = 516,\n",
    "              max_epochs=10000,\n",
    "              eval_metric=['mse'])\n",
    "\n",
    "lt_predictions = lt_regressor.predict(lt_X_test)\n",
    "\n",
    "print('BEST VALID SCORE : ', lt_regressor.best_cost)\n",
    "print('R2 SCORE : ' , r2_score(scaled_lt_y_test, lt_predictions))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(scaled_lt_y_test[:,0], lt_predictions[:,0])) #TPS\n",
    "print(r2_score(scaled_lt_y_test[:,1], lt_predictions[:,1])) #LATENCY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_pd = pd.DataFrame(encoded_vector_BO.cpu().numpy())\n",
    "latent_pd_T = latent_pd.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bayesian Optimization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "class BO(object):\n",
    "    def __init__(self, iteration, configs, metrics, regressor,config_info_path=None,):\n",
    "        self.iteration = iteration\n",
    "        self.configs = configs\n",
    "        self.metrics = metrics\n",
    "        self.config_info_path = config_info_path\n",
    "        self.regressor = regressor\n",
    "        self._get_config_info()\n",
    "        self._init_pbounds()\n",
    "    \n",
    "    def _get_config_info(self):\n",
    "        if self.config_info_path is None:\n",
    "            self.config_info = pd.read_csv('/home/Knob_Information_MySQL_v5.7.csv', index_col=0)\n",
    "        else:\n",
    "            self.config_info = pd.read_csv(self.config_info_path, index_col=0)\n",
    "\n",
    "    \n",
    "    def _init_pbounds(self):\n",
    "        self.pbounds = {}\n",
    "        \n",
    "        for v in latent_pd_T.index:\n",
    "            self.pbounds[str(v)] = (0, 1)\n",
    "\n",
    "    \n",
    "    def _target_function(self, **kwargs):\n",
    "        x = np.fromiter(kwargs.values(), dtype=float)        \n",
    "        x = x.reshape(1, -1)\n",
    "\n",
    "        res = self.regressor.predict(x)\n",
    "        res = res[:,0] / res[:,1]\n",
    "\n",
    "        \n",
    "        return res.squeeze()\n",
    "    \n",
    "    \n",
    "    def tune(self):\n",
    "        self.optimizer = BayesianOptimization(f=self._target_function, pbounds=self.pbounds, verbose=2, random_state=2)\n",
    "\n",
    "        self.acquisition_function = UtilityFunction(kind=\"ei\", kappa=2.5, xi=0.001)\n",
    "        \n",
    "        self.optimizer.maximize(n_iter=self.iteration, init_points=150, acquisition_function=self.acquisition_function)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner1 = BO(iteration=1000, \n",
    "           configs=encoded_vector_BO.cpu().numpy(),\n",
    "           metrics=new_metrics_re,\n",
    "           regressor=lt_regressor\n",
    "           )\n",
    "\n",
    "tuner1.tune()\n",
    "best = tuner1.optimizer.max #max of the tuned config's score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_data = list(best['params'].values())\n",
    "ex_data = [round(val, 4) for val in best['params'].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding to real dimension\n",
    "\n",
    "ex_data = torch.tensor(ex_data).unsqueeze(0)\n",
    "ex_data = ex_data.to('cuda:0')  \n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    decode_value = model.decoder(ex_data)\n",
    "    print(\"Decoded Value:\", decode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_values = [float(value) for value in decode_value[0]]\n",
    "\n",
    "df_converted_values = pd.DataFrame(converted_values)\n",
    "\n",
    "real_bo_config = df_converted_values[:138] \n",
    "real_v = np.array(real_bo_config)\n",
    "\n",
    "rescaled_bo_config = scaler_conf.inverse_transform(real_v.reshape(1,-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.9f}'.format \n",
    "\n",
    "rescaled_actual_pd = pd.DataFrame(rescaled_bo_config)\n",
    "rescaled_actual_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(A_config.columns)):\n",
    "    print('{} = {}'.format(A_config.columns[i], round(rescaled_actual_pd.iloc[0][i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
